{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=500\n",
    "train_dataloader = DataLoader(train_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 'cuda:1'\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "          \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 81 ==> 8\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(81, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 32)\n",
    "        )\n",
    "          \n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 8 ==> 81\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 81),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2093033/4016921915.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_w=np.array(self.language_sdk[hid]['context_embedding_indexes'])\n",
      "/tmp/ipykernel_2093033/4016921915.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_of=np.array(self.word_aligned_openface_sdk[hid]['context_features'])\n",
      "/tmp/ipykernel_2093033/4016921915.py:104: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_cvp=np.array(self.word_aligned_covarep_sdk[hid]['context_features'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.04964252561330795\n",
      "2 0.02786126174032688\n",
      "3 0.023423954844474792\n",
      "4 0.02129523828625679\n",
      "5 0.017899321392178535\n",
      "6 0.015334535390138626\n",
      "7 0.013559437356889248\n",
      "8 0.01310691051185131\n",
      "9 0.014500143006443977\n",
      "10 0.012951018288731575\n",
      "11 0.011947487480938435\n",
      "12 0.013090623542666435\n",
      "13 0.012150808237493038\n",
      "14 0.012081514112651348\n",
      "15 0.0122918039560318\n",
      "16 0.01198199950158596\n",
      "17 0.012114358134567738\n",
      "18 0.011589252389967442\n",
      "19 0.0107578095048666\n",
      "20 0.011263806372880936\n",
      "21 0.010502317920327187\n",
      "22 0.011069527827203274\n",
      "23 0.010378207080066204\n",
      "24 0.00938884075731039\n",
      "25 0.010464576072990894\n",
      "26 0.009104632772505283\n",
      "27 0.008910419419407845\n",
      "28 0.008526005782186985\n",
      "29 0.008994536474347115\n",
      "30 0.008137205615639687\n",
      "31 0.007479557767510414\n",
      "32 0.007106035947799683\n",
      "33 0.007603404112160206\n",
      "34 0.007432801648974419\n",
      "35 0.007160036358982325\n",
      "36 0.007108702789992094\n",
      "37 0.00673686433583498\n",
      "38 0.007435251027345657\n",
      "39 0.0063443537801504135\n",
      "40 0.006681784987449646\n",
      "41 0.006714762654155493\n",
      "42 0.005483944434672594\n",
      "43 0.006450146436691284\n",
      "44 0.006129590794444084\n",
      "45 0.006153710652142763\n",
      "46 0.005781461484730244\n",
      "47 0.005944158416241407\n",
      "48 0.005624071229249239\n",
      "49 0.0058646961115300655\n",
      "50 0.006130399648100138\n",
      "51 0.0053630247712135315\n",
      "52 0.005038829520344734\n",
      "53 0.004884842783212662\n",
      "54 0.0055763004347682\n",
      "55 0.005712485406547785\n",
      "56 0.005046403035521507\n",
      "57 0.0055121141485869884\n",
      "58 0.005472153425216675\n",
      "59 0.004927163477987051\n",
      "60 0.004487727768719196\n",
      "61 0.0052015818655490875\n",
      "62 0.0051003796979784966\n",
      "63 0.004588570911437273\n",
      "64 0.005041480530053377\n",
      "65 0.005157426930963993\n",
      "66 0.0046448782086372375\n",
      "67 0.00482770474627614\n",
      "68 0.004250506870448589\n",
      "69 0.004784197546541691\n",
      "70 0.004245318006724119\n",
      "71 0.00545877031981945\n",
      "72 0.0044235228560864925\n",
      "73 0.0044495705515146255\n",
      "74 0.003908069338649511\n",
      "75 0.003993456717580557\n",
      "76 0.004145863000303507\n",
      "77 0.004157496616244316\n",
      "78 0.0041041187942028046\n",
      "79 0.003773591946810484\n",
      "80 0.004369134549051523\n",
      "81 0.004489121027290821\n",
      "82 0.004041491542011499\n",
      "83 0.004186284262686968\n",
      "84 0.0037494651041924953\n",
      "85 0.00399628933519125\n",
      "86 0.0040383716113865376\n",
      "87 0.003667236538603902\n",
      "88 0.0038948918227106333\n",
      "89 0.0038081940729171038\n",
      "90 0.0038212225772440434\n",
      "91 0.004043548833578825\n",
      "92 0.003933942411094904\n",
      "93 0.003722455818206072\n",
      "94 0.003770149778574705\n",
      "95 0.003551843808963895\n",
      "96 0.0038173608481884003\n",
      "97 0.0038375339936465025\n",
      "98 0.003734682919457555\n",
      "99 0.003461031708866358\n",
      "100 0.0036695904564112425\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2093033/1777841782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (100,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input features\n",
    "###\n",
    "#idxs = torch.arange(300) # text, 300\n",
    "idxs = torch.arange(300, 381) # speech, 81\n",
    "#idxs = torch.arange(381, 752) # visual, 371\n",
    "#idxs = torch.arange(381) # text + speech, 381\n",
    "#idxs = torch.cat((torch.arange(300), torch.arange(381, 752))) # text + visual, 671\n",
    "#idxs = torch.arange(300, 752) # speech + visual, 452\n",
    "#idxs = torch.arange(752) # text + speech + visual, 752\n",
    "\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')\n",
    "ae = AE()\n",
    "ae.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(ae.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "print('step', 'error')\n",
    "errors = []\n",
    "best_error = np.inf\n",
    "num_bad_epochs = 0\n",
    "num_epochs = 0\n",
    "best_epoch = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    for batch_idx, batch in enumerate(train_dataloader, 0):\n",
    "        x_p,x_c,y,hid,x_p_len=map(lambda x: x.to(device), batch)\n",
    "        speech_batch = []\n",
    "        for i, sent in enumerate(x_p):\n",
    "            for j, word in enumerate(sent):\n",
    "                if j < x_p_len[i]:\n",
    "                    speech_batch.append(word[torch.arange(300, 381)].detach().tolist())\n",
    "        speech_batch = torch.tensor(speech_batch, dtype=torch.float32, device=device)\n",
    "        optimiser.zero_grad()\n",
    "        _, output = ae(speech_batch)\n",
    "        error = torch.nn.functional.mse_loss(output, speech_batch)\n",
    "        error.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    error = error.detach().tolist()\n",
    "    print(epoch, error)\n",
    "    errors.append(error)\n",
    "\n",
    "    if error < best_error:\n",
    "        num_bad_epochs = 0\n",
    "        best_error = error\n",
    "        best_epoch = epoch\n",
    "        torch.save(ae, 'autoencoder_speech.pth')\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "        if num_bad_epochs == patience:\n",
    "            num_epochs = epoch\n",
    "            break\n",
    "\n",
    "plt.plot(range(1, num_epochs+1), errors, 'r')\n",
    "plt.title('train error')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n",
    "\n",
    "print('best model after {} epochs with error {}'.format(best_epoch, best_error))\n",
    "ae = torch.load('autoencoder_speech.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = torch.load('autoencoder_speech.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2158469/4016921915.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_w=np.array(self.language_sdk[hid]['context_embedding_indexes'])\n",
      "/tmp/ipykernel_2158469/4016921915.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_of=np.array(self.word_aligned_openface_sdk[hid]['context_features'])\n",
      "/tmp/ipykernel_2158469/4016921915.py:104: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_cvp=np.array(self.word_aligned_covarep_sdk[hid]['context_features'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 1.0000e+00, 5.6451e-01, 6.2728e-01, 2.0565e-02, 4.7851e-01,\n",
      "        6.2589e-01, 1.9001e-02, 5.0860e-01, 5.4266e-01, 4.9736e-01, 4.9058e-01,\n",
      "        4.8361e-01, 4.7963e-01, 4.8082e-01, 4.8842e-01, 4.9828e-01, 5.0074e-01,\n",
      "        5.7610e-01, 5.5998e-01, 5.3670e-01, 5.1315e-01, 4.8905e-01, 4.8725e-01,\n",
      "        4.8440e-01, 4.9080e-01, 5.0801e-01, 5.2765e-01, 5.4929e-01, 5.6546e-01,\n",
      "        4.9206e-01, 4.9124e-01, 4.8828e-01, 4.8502e-01, 4.8349e-01, 4.8450e-01,\n",
      "        4.8729e-01, 4.9041e-01, 2.2663e-01, 2.2904e-01, 2.2806e-01, 1.9800e-01,\n",
      "        1.7907e-01, 1.7838e-01, 1.9781e-01, 2.1652e-01, 2.4042e-01, 2.1978e-01,\n",
      "        1.9394e-01, 1.7205e-01, 1.5576e-01, 1.5095e-01, 1.4584e-01, 1.4591e-01,\n",
      "        1.5359e-01, 1.6855e-01, 1.9176e-01, 2.1929e-01, 2.2006e-01, 2.0532e-01,\n",
      "        1.9617e-01, 1.9649e-01, 2.0570e-01, 2.1988e-01, 2.2911e-01, 2.2787e-01,\n",
      "        5.7278e-01, 5.8348e-01, 6.0078e-01, 6.1620e-01, 6.0813e-01, 5.9985e-01,\n",
      "        5.8537e-01, 5.7310e-01, 5.5589e-01, 5.6956e-01, 5.7676e-01, 5.8932e-01,\n",
      "        5.9073e-01, 5.9161e-01, 5.9297e-01, 5.9500e-01, 5.9461e-01, 5.8673e-01,\n",
      "        5.6592e-01, 5.5604e-01, 5.7669e-01, 5.8248e-01, 5.9100e-01, 5.9690e-01,\n",
      "        5.9631e-01, 5.8982e-01, 5.8178e-01, 5.7651e-01, 5.0782e-01, 5.9054e-01,\n",
      "        6.7901e-01, 6.9816e-01, 6.4987e-01, 5.6465e-01, 4.9795e-01, 4.5877e-01,\n",
      "        5.4386e-01, 5.6019e-01, 6.0087e-01, 6.0211e-01, 5.9051e-01, 5.7238e-01,\n",
      "        5.4033e-01, 5.1831e-01, 5.2717e-01, 5.4312e-01, 5.3826e-01, 5.3318e-01,\n",
      "        5.2396e-01, 5.3796e-01, 5.6724e-01, 6.0284e-01, 6.2547e-01, 6.1048e-01,\n",
      "        5.7469e-01, 5.3917e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01,\n",
      "        2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01,\n",
      "        2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01,\n",
      "        2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01,\n",
      "        2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01, 2.7587e-01,\n",
      "        9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01,\n",
      "        9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01,\n",
      "        9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01,\n",
      "        9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01,\n",
      "        9.5632e-01, 9.5632e-01, 9.5632e-01, 9.5632e-01, 7.0188e-01, 7.0188e-01,\n",
      "        7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01,\n",
      "        7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01,\n",
      "        7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01,\n",
      "        7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01, 7.0188e-01,\n",
      "        7.0188e-01, 7.0188e-01, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02,\n",
      "        2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02,\n",
      "        2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02,\n",
      "        2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02,\n",
      "        2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02, 2.1456e-02,\n",
      "        2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01,\n",
      "        2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01,\n",
      "        2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01,\n",
      "        2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01,\n",
      "        2.8358e-01, 2.8358e-01, 2.8358e-01, 2.8358e-01, 9.5238e-01, 9.5238e-01,\n",
      "        9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01,\n",
      "        9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01,\n",
      "        9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01,\n",
      "        9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01, 9.5238e-01,\n",
      "        9.5238e-01, 9.5238e-01, 1.0187e-01, 9.9959e-01, 5.7491e-04, 5.3581e-01,\n",
      "        5.4297e-01, 4.9729e-01, 1.2842e-01, 5.1671e-01, 6.4140e-01, 4.8791e-01,\n",
      "        3.4299e-01, 5.7946e-01, 4.2608e-01, 4.1730e-01, 4.4416e-01, 4.0042e-01,\n",
      "        4.4489e-01, 3.0522e-01, 2.9105e-01, 6.3061e-01, 6.7451e-01, 3.4167e-01,\n",
      "        5.9530e-01, 4.5664e-01, 5.1069e-01, 5.2713e-01, 3.1917e-01, 4.1409e-01,\n",
      "        6.5138e-01, 2.8919e-01, 5.4396e-01, 3.7865e-01, 2.9757e-01, 6.7883e-01,\n",
      "        2.5319e-01, 6.1903e-01, 6.1575e-01, 5.7265e-01, 5.0491e-01, 7.8031e-01,\n",
      "        2.8763e-01, 4.2669e-01, 3.4953e-01, 4.8410e-01, 3.4556e-01, 2.2046e-01,\n",
      "        2.8000e-02, 0.0000e+00, 1.7000e-01, 0.0000e+00, 0.0000e+00, 2.1200e-01,\n",
      "        4.0000e-02, 3.3800e-01, 5.6471e-02, 0.0000e+00, 0.0000e+00, 1.5400e-01,\n",
      "        0.0000e+00, 4.6000e-02, 3.0000e-01, 4.7400e-01, 4.0000e-02, 1.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_dataloader, 0):\n",
    "    x_p,x_c,y,hid,x_p_len=map(lambda x: x.to('cuda:1'), batch)\n",
    "    speech_batch = []\n",
    "    for i, sent in enumerate(x_p):\n",
    "        for j, word in enumerate(sent):\n",
    "            if j < x_p_len[i]:\n",
    "                speech_batch.append(word[torch.arange(381, 752)].detach().tolist())\n",
    "    speech_batch = torch.tensor(speech_batch, dtype=torch.float32, device='cuda:1')\n",
    "    print(speech_batch[0])\n",
    "    #print(ae(speech_batch[0])[1])\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
