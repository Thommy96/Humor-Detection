{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=500\n",
    "train_dataloader = DataLoader(train_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 'cuda:1'\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "          \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 81 ==> 8\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(752, 856),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(856, 1024)\n",
    "        )\n",
    "          \n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 8 ==> 81\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1024, 856),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(856, 752),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2159175/4016921915.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_w=np.array(self.language_sdk[hid]['context_embedding_indexes'])\n",
      "/tmp/ipykernel_2159175/4016921915.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_of=np.array(self.word_aligned_openface_sdk[hid]['context_features'])\n",
      "/tmp/ipykernel_2159175/4016921915.py:104: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_cvp=np.array(self.word_aligned_covarep_sdk[hid]['context_features'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.060116421431303024\n",
      "2 0.04257666692137718\n",
      "3 0.04062328487634659\n",
      "4 0.040538087487220764\n",
      "5 0.03969992324709892\n",
      "6 0.038339436054229736\n",
      "7 0.03792921081185341\n",
      "8 0.0379788912832737\n",
      "9 0.03732626885175705\n",
      "10 0.0370081290602684\n",
      "11 0.03813313692808151\n",
      "12 0.03730037808418274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2159175/2018921939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mx_p_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mspeech_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mspeech_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input features\n",
    "###\n",
    "#idxs = torch.arange(300) # text, 300\n",
    "#idxs = torch.arange(300, 381) # speech, 81\n",
    "#idxs = torch.arange(381, 752) # visual, 371\n",
    "#idxs = torch.arange(381) # text + speech, 381\n",
    "#idxs = torch.cat((torch.arange(300), torch.arange(381, 752))) # text + visual, 671\n",
    "#idxs = torch.arange(300, 752) # speech + visual, 452\n",
    "idxs = torch.arange(752) # text + speech + visual, 752\n",
    "\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')\n",
    "ae = AE()\n",
    "ae.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(ae.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "epochs = 50\n",
    "patience = 20\n",
    "\n",
    "print('step', 'error')\n",
    "errors = []\n",
    "best_error = np.inf\n",
    "num_bad_epochs = 0\n",
    "num_epochs = 0\n",
    "best_epoch = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    for batch_idx, batch in enumerate(train_dataloader, 0):\n",
    "        x_p,x_c,y,hid,x_p_len=map(lambda x: x.to(device), batch)\n",
    "        speech_batch = []\n",
    "        for i, sent in enumerate(x_p):\n",
    "            for j, word in enumerate(sent):\n",
    "                if j < x_p_len[i]:\n",
    "                    speech_batch.append(word[idxs].detach().tolist())\n",
    "        speech_batch = torch.tensor(speech_batch, dtype=torch.float32, device=device)\n",
    "        optimiser.zero_grad()\n",
    "        _, output = ae(speech_batch)\n",
    "        error = torch.nn.functional.mse_loss(output, speech_batch)\n",
    "        error.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    error = error.detach().tolist()\n",
    "    print(epoch, error)\n",
    "    errors.append(error)\n",
    "\n",
    "    if error < best_error:\n",
    "        num_bad_epochs = 0\n",
    "        best_error = error\n",
    "        best_epoch = epoch\n",
    "        num_epochs = epoch\n",
    "        torch.save(ae, 'autoencoder_all_2.pth')\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "        num_epochs = epoch\n",
    "        if num_bad_epochs == patience:\n",
    "            break\n",
    "\n",
    "plt.plot(range(1, num_epochs+1), errors, 'r')\n",
    "plt.title('train error')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n",
    "\n",
    "print('best model after {} epochs with error {}'.format(best_epoch, best_error))\n",
    "ae = torch.load('autoencoder_all_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2159175/4016921915.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_w=np.array(self.language_sdk[hid]['context_embedding_indexes'])\n",
      "/tmp/ipykernel_2159175/4016921915.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_of=np.array(self.word_aligned_openface_sdk[hid]['context_features'])\n",
      "/tmp/ipykernel_2159175/4016921915.py:104: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context_cvp=np.array(self.word_aligned_covarep_sdk[hid]['context_features'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1076e-01,  3.0786e-01, -5.1980e-01,  3.5138e-02,  1.0368e-01,\n",
      "        -5.2505e-02, -1.8021e-01, -1.1839e-01, -5.4253e-02,  2.4980e+00,\n",
      "        -3.0241e-01,  4.3233e-02, -9.5862e-02, -9.3529e-02, -1.9817e-01,\n",
      "        -2.6599e-01, -3.4703e-01,  1.4518e+00, -4.9013e-01,  4.1637e-02,\n",
      "         1.1185e-01, -1.9023e-02, -1.8716e-01, -1.0407e-01, -4.3665e-01,\n",
      "         7.3561e-02,  1.9546e-02, -1.5012e-01,  1.8499e-01, -2.4364e-01,\n",
      "         2.0327e-01,  2.8916e-01, -2.1694e-01,  2.8351e-01, -1.0092e-01,\n",
      "        -4.2189e-02, -7.3457e-02,  2.7325e-01, -1.2898e-01, -5.9407e-02,\n",
      "        -7.3329e-02,  1.2490e-02, -2.0459e-01, -4.4558e-01,  4.0863e-02,\n",
      "         2.4588e-01, -2.6111e-01, -8.6821e-02,  1.3628e-01,  1.1094e-01,\n",
      "        -1.0835e-01,  9.8775e-03,  1.7394e-01,  6.4750e-03,  2.7467e-01,\n",
      "        -9.7433e-03,  1.6561e-01, -1.6975e-01, -1.2561e-01, -7.1688e-02,\n",
      "        -5.6815e-02, -2.8632e-01, -2.4231e-01,  2.7819e-01,  2.4112e-01,\n",
      "        -9.1420e-03, -5.3634e-02,  4.3907e-01,  3.9000e-01,  1.2520e-01,\n",
      "        -6.3581e-02,  5.8089e-02,  5.9187e-01, -1.8385e-01,  9.0201e-02,\n",
      "         1.3788e-01,  4.1051e-01, -3.9034e-01, -7.1701e-02,  3.7935e-01,\n",
      "         3.1344e-02, -3.6150e-03, -2.5773e-01, -4.8608e-02,  1.9520e-01,\n",
      "        -2.9912e-01,  4.7210e-02, -1.3577e-01,  6.7253e-01, -8.3033e-02,\n",
      "        -1.9680e-01,  7.4079e-02,  1.7826e-01,  2.0097e-01, -3.6357e-02,\n",
      "         2.7783e-02, -3.2144e-01, -2.9620e-01, -1.3260e-01,  3.0375e-01,\n",
      "         5.4180e-02,  7.0012e-02,  1.1935e-01,  4.6680e-02,  3.7338e-01,\n",
      "        -6.3809e-01,  3.3868e-01, -9.1924e-02, -1.2639e-01,  6.8526e-02,\n",
      "         1.1981e-01, -2.2509e-01,  5.6067e-01, -3.5003e-02,  3.6471e-01,\n",
      "        -2.6875e-01, -4.8343e-03,  6.4098e-02, -2.8760e-01, -2.3736e-02,\n",
      "         2.1348e-01, -4.1220e-01, -1.2958e-01,  5.1024e-02,  4.2078e-01,\n",
      "        -8.6314e-02, -1.0035e-01, -2.6017e-01,  9.6791e-03,  6.4299e-02,\n",
      "         1.0799e-01, -9.5081e-02, -1.2798e-01,  5.4993e-02,  6.0576e-02,\n",
      "        -3.7241e-02, -1.9778e-01, -1.2237e-01, -1.6846e-01, -9.8457e-02,\n",
      "        -1.8562e+00,  3.1190e-01, -3.0854e-01, -9.8816e-02, -1.9955e-03,\n",
      "        -2.9415e-01,  7.8162e-02,  1.8014e-01, -2.7904e-02, -4.9573e-02,\n",
      "         7.1973e-02,  1.6791e-01, -3.3054e-02, -7.9709e-02, -9.7695e-02,\n",
      "         2.6119e-01,  1.1585e-01, -2.5638e-01, -8.9019e-02, -2.4823e-02,\n",
      "        -1.0813e-01,  2.0349e-01, -2.0903e-01,  1.8039e-01,  3.9647e-01,\n",
      "        -1.3119e-01,  4.6686e-01, -5.3135e-02,  1.4807e-02,  5.9119e-02,\n",
      "        -8.4577e-02, -5.8610e-02,  3.4677e-01, -2.5996e-01,  5.2293e-02,\n",
      "         1.9285e-01, -2.7362e-01, -1.0858e-01, -3.0143e-02,  3.5079e-01,\n",
      "         2.0094e-01,  8.7390e-02, -1.2402e-01,  2.0940e-02,  4.1557e-02,\n",
      "        -2.6728e-02, -2.5289e-02, -3.4984e-01, -7.8001e-02,  1.7182e-01,\n",
      "        -6.2930e-02, -7.4751e-02,  4.5825e-02, -2.7333e-01,  2.3052e-01,\n",
      "         1.9061e-01, -2.0641e-01, -3.9203e-02,  3.3908e-01,  5.2254e-01,\n",
      "        -1.0861e-01, -3.0465e-01, -5.3306e-02, -2.6766e-01, -4.3355e-03,\n",
      "         2.3916e-01,  2.2283e-01, -5.3289e-02,  2.0198e-01, -8.4151e-02,\n",
      "         1.0375e-01, -3.5093e-01, -1.9961e-01,  1.0933e-02,  2.6317e-01,\n",
      "         3.4094e-01, -6.8638e-02,  2.0576e-01, -5.2757e-01, -8.4815e-02,\n",
      "         1.1056e-01,  2.1289e-02,  6.3286e-02,  9.4234e-02,  2.0282e-01,\n",
      "        -1.5887e-01, -1.0649e-02,  2.5771e-01, -2.3234e-01, -2.3733e-01,\n",
      "        -1.5439e-01,  1.3906e-01,  8.6255e-02,  3.8443e-01, -2.5632e-01,\n",
      "         3.1801e-02,  8.0305e-02, -4.0683e-01, -5.1163e-01,  2.6979e-01,\n",
      "         4.1308e-01,  5.7052e-02,  5.4701e-02, -6.0832e-02,  1.9468e-01,\n",
      "        -3.8259e-01, -4.4148e-02, -3.6737e-02, -3.9720e-01,  5.5777e-01,\n",
      "         6.9855e-02, -2.1519e-01, -9.1095e-02,  3.3559e-02, -1.6332e-01,\n",
      "         4.2089e-01,  1.9067e-02, -2.1884e-01,  2.7533e-01,  2.3683e-01,\n",
      "         9.4193e-02,  3.8504e-02,  2.2380e-01, -1.1986e-01,  2.3199e-01,\n",
      "        -8.8445e-02, -1.4716e-02,  6.5752e-01,  5.9385e-01,  2.4571e-01,\n",
      "         2.4754e-02, -3.1514e-01, -1.5470e-01,  5.7218e-04, -4.2344e-02,\n",
      "         8.1696e-02,  3.0109e-02,  7.0089e-02,  8.7080e-02, -7.9636e-02,\n",
      "        -8.3257e-03, -1.4395e-01,  3.8982e-02, -9.5362e-02,  2.7599e-01,\n",
      "        -3.9070e-01,  4.4441e-01, -3.5471e-01,  2.3310e-01, -6.7546e-03,\n",
      "        -1.8892e-01,  2.7837e-01, -3.8501e-01, -1.1408e-01,  2.8191e-01,\n",
      "        -3.0946e-01, -2.1878e-01, -5.9105e-02,  4.7604e-01,  5.6610e-02,\n",
      "         3.2056e-01,  6.6667e-01,  6.5810e-02,  1.8430e-03,  4.8489e-01,\n",
      "         1.1678e-02,  7.9728e-01,  1.2539e-01,  8.9466e-01,  6.9816e-01,\n",
      "         5.8027e-01,  5.7312e-01,  6.7015e-01,  5.6374e-01,  5.2177e-01,\n",
      "         5.6170e-01,  5.2557e-01,  5.2730e-01,  4.4154e-01,  2.8830e-01,\n",
      "         5.3660e-01,  5.0440e-01,  4.5367e-01,  4.4185e-01,  4.0050e-01,\n",
      "         5.3395e-01,  5.4780e-01,  5.6591e-01,  4.7600e-01,  4.0737e-01,\n",
      "         5.5822e-01,  5.3265e-01,  4.4957e-01,  5.0889e-01,  4.6909e-01,\n",
      "         5.6007e-01,  8.3713e-01,  2.3225e-01,  4.8886e-01,  4.9556e-01,\n",
      "         5.1796e-01,  5.6620e-01,  6.6057e-01,  7.9162e-01,  8.1599e-01,\n",
      "         8.4747e-01,  9.2062e-01,  9.1081e-01,  7.9593e-01,  8.4036e-01,\n",
      "         8.3053e-01,  7.7232e-01,  8.1522e-01,  8.9070e-01,  6.5065e-01,\n",
      "         6.6009e-01,  9.9542e-01,  5.1214e-02,  5.4746e-02,  1.2412e-01,\n",
      "         8.9667e-01,  9.6166e-01,  3.6336e-01,  3.0593e-01,  2.6568e-01,\n",
      "         1.8843e-01,  1.9771e-01,  1.7202e-01,  2.4187e-01,  2.8731e-01,\n",
      "         2.5580e-01,  3.1934e-01,  3.3978e-01,  3.2036e-01,  0.0000e+00,\n",
      "         2.2661e-01,  2.8969e-01,  3.7658e-01,  4.4345e-01,  5.7451e-01,\n",
      "         2.1000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:1')\n",
      "tensor([-1.1295e-01,  3.4961e-01, -4.8519e-01,  3.8293e-02,  1.1144e-01,\n",
      "        -7.6408e-02, -1.7525e-01, -9.8892e-02, -1.0214e-02,  2.4626e+00,\n",
      "        -3.2077e-01,  3.8675e-02, -1.3709e-01, -1.0210e-01, -1.8869e-01,\n",
      "        -2.6106e-01, -3.2410e-01,  1.4383e+00, -4.7352e-01,  2.1035e-02,\n",
      "         1.1498e-01, -5.4194e-02, -1.8194e-01, -1.1842e-01, -3.9134e-01,\n",
      "         1.0597e-01,  1.8076e-03, -1.6206e-01,  1.6772e-01, -2.5243e-01,\n",
      "         1.8733e-01,  2.6058e-01, -1.8666e-01,  2.8134e-01, -5.7984e-02,\n",
      "        -5.7002e-03, -4.1924e-02,  2.6076e-01, -1.1253e-01, -7.0192e-02,\n",
      "        -1.0185e-01, -1.2330e-02, -1.9086e-01, -4.2788e-01,  3.9997e-02,\n",
      "         1.9056e-01, -2.7754e-01, -7.7326e-02,  1.1836e-01,  1.2883e-01,\n",
      "        -8.6215e-02, -4.7839e-03,  1.8437e-01, -1.6140e-03,  2.8887e-01,\n",
      "        -2.5332e-02,  1.7247e-01, -1.5415e-01, -1.0008e-01, -5.9471e-02,\n",
      "        -5.2063e-02, -2.4924e-01, -2.5798e-01,  2.8234e-01,  2.7638e-01,\n",
      "        -2.2247e-02, -7.8015e-02,  4.1469e-01,  3.9162e-01,  1.4125e-01,\n",
      "        -5.7217e-02,  4.0699e-02,  5.6888e-01, -1.7959e-01,  6.3511e-02,\n",
      "         1.4097e-01,  3.9807e-01, -3.8979e-01, -6.1334e-02,  3.4504e-01,\n",
      "         3.0369e-02,  4.7914e-02, -2.3741e-01, -4.7773e-02,  1.7371e-01,\n",
      "        -3.0051e-01,  4.6634e-02, -1.4679e-01,  6.6474e-01, -1.0794e-01,\n",
      "        -1.7883e-01,  1.1943e-01,  1.1722e-01,  2.1750e-01, -4.7127e-02,\n",
      "         3.8821e-02, -3.4553e-01, -3.0276e-01, -1.4523e-01,  2.7152e-01,\n",
      "         5.7105e-02,  7.4288e-02,  1.3117e-01,  4.2048e-02,  3.5295e-01,\n",
      "        -6.3517e-01,  3.1734e-01, -9.4205e-02, -1.3576e-01,  9.0450e-02,\n",
      "         1.3091e-01, -2.5026e-01,  5.4574e-01, -3.2929e-02,  3.3899e-01,\n",
      "        -2.0983e-01, -2.9318e-02,  6.1743e-02, -2.9326e-01, -4.5763e-02,\n",
      "         1.6926e-01, -4.2252e-01, -1.4124e-01,  5.8450e-02,  3.8717e-01,\n",
      "        -1.2684e-01, -1.0477e-01, -2.9604e-01,  1.1142e-02,  3.6660e-02,\n",
      "         9.8149e-02, -1.0257e-01, -9.8139e-02,  8.4580e-02,  7.2548e-02,\n",
      "        -3.6227e-02, -1.9041e-01, -1.4333e-01, -1.4863e-01, -1.0649e-01,\n",
      "        -1.8312e+00,  3.3117e-01, -2.9536e-01, -4.7969e-02,  3.1441e-02,\n",
      "        -2.8796e-01,  9.1801e-02,  2.0023e-01, -5.7659e-03, -2.8512e-02,\n",
      "         9.7599e-02,  1.2934e-01, -1.4622e-02, -6.6976e-02, -7.2068e-02,\n",
      "         2.7121e-01,  1.0524e-01, -2.4046e-01, -1.1310e-01, -2.6909e-02,\n",
      "        -1.1143e-01,  2.2815e-01, -2.3915e-01,  1.6657e-01,  4.5491e-01,\n",
      "        -1.2874e-01,  4.6619e-01, -3.1605e-02,  9.3341e-03,  7.9660e-02,\n",
      "        -1.1159e-01, -7.9095e-02,  3.3426e-01, -2.7645e-01,  4.3720e-02,\n",
      "         1.8887e-01, -2.6599e-01, -1.2370e-01, -8.1458e-02,  3.5417e-01,\n",
      "         2.0260e-01,  6.7081e-02, -1.0670e-01,  2.8007e-02,  5.9532e-02,\n",
      "         6.1165e-03, -2.4089e-02, -3.3584e-01, -4.7586e-02,  1.7789e-01,\n",
      "        -7.3830e-02, -9.7337e-02,  5.9736e-02, -2.5936e-01,  2.2572e-01,\n",
      "         1.8735e-01, -2.0200e-01, -5.4027e-02,  3.4735e-01,  5.0353e-01,\n",
      "        -1.1924e-01, -2.6788e-01, -5.5389e-02, -2.7284e-01,  2.5864e-03,\n",
      "         2.3616e-01,  2.2234e-01, -3.0740e-02,  2.0423e-01, -6.1308e-02,\n",
      "         5.4064e-02, -3.9500e-01, -1.9993e-01, -1.8066e-02,  2.7688e-01,\n",
      "         2.9473e-01, -6.7246e-02,  2.2396e-01, -5.3476e-01, -1.0998e-01,\n",
      "         1.1084e-01,  1.9189e-02,  4.4842e-02,  5.5290e-02,  2.0972e-01,\n",
      "        -1.6375e-01, -2.6665e-02,  2.5039e-01, -2.4022e-01, -2.1264e-01,\n",
      "        -1.7577e-01,  1.5285e-01,  1.0807e-01,  3.5075e-01, -2.7902e-01,\n",
      "         2.4451e-02,  9.7904e-02, -3.8864e-01, -5.1426e-01,  2.7371e-01,\n",
      "         4.2929e-01,  4.3895e-02,  4.0359e-02, -5.3763e-02,  1.9264e-01,\n",
      "        -3.9204e-01, -7.2646e-02, -6.7222e-02, -3.7036e-01,  5.3343e-01,\n",
      "         1.0752e-01, -2.2760e-01, -1.1674e-01,  2.1805e-02, -1.8298e-01,\n",
      "         4.4065e-01,  3.9367e-02, -2.1194e-01,  2.9823e-01,  1.9540e-01,\n",
      "         8.4370e-02,  2.6598e-02,  2.1815e-01, -1.3347e-01,  2.6232e-01,\n",
      "        -1.0493e-01, -7.8947e-02,  6.0527e-01,  5.7181e-01,  2.6031e-01,\n",
      "         1.2901e-03, -3.2109e-01, -1.2357e-01,  4.8713e-03, -4.0584e-02,\n",
      "         8.3951e-02,  2.3202e-02,  9.0238e-02,  8.5024e-02, -5.1380e-02,\n",
      "        -4.6229e-02, -1.3408e-01, -1.1116e-02, -7.3243e-02,  2.4549e-01,\n",
      "        -3.6186e-01,  4.5436e-01, -3.6358e-01,  2.6251e-01, -1.7498e-02,\n",
      "        -1.7869e-01,  2.5694e-01, -3.4778e-01, -1.1805e-01,  3.0119e-01,\n",
      "        -2.8871e-01, -2.4350e-01, -7.0254e-02,  4.5138e-01,  4.0948e-02,\n",
      "         3.7258e-01,  7.7120e-01,  1.5519e-01,  1.2468e-03,  5.7145e-01,\n",
      "         2.5080e-02,  7.8940e-01,  1.0844e-01,  9.0538e-01,  6.4151e-01,\n",
      "         6.0427e-01,  6.1746e-01,  6.8632e-01,  4.9562e-01,  5.2420e-01,\n",
      "         4.9681e-01,  5.7756e-01,  5.2118e-01,  4.6000e-01,  4.3481e-01,\n",
      "         5.1643e-01,  5.2771e-01,  4.7673e-01,  4.3891e-01,  4.8045e-01,\n",
      "         4.9164e-01,  5.1632e-01,  4.8785e-01,  4.7709e-01,  5.3589e-01,\n",
      "         4.8292e-01,  5.0575e-01,  4.6992e-01,  4.5242e-01,  5.2085e-01,\n",
      "         4.6468e-01,  8.2644e-01,  2.3347e-01,  4.8183e-01,  4.9763e-01,\n",
      "         5.1898e-01,  5.5628e-01,  5.9899e-01,  6.6730e-01,  7.2240e-01,\n",
      "         7.8986e-01,  8.1252e-01,  8.1506e-01,  8.1438e-01,  8.2203e-01,\n",
      "         8.2642e-01,  8.1451e-01,  8.1762e-01,  8.0197e-01,  7.3361e-01,\n",
      "         7.0697e-01,  7.1085e-01,  6.9277e-01,  5.7797e-01,  4.0198e-01,\n",
      "         3.7209e-01,  9.5264e-01,  3.5743e-01,  3.1563e-01,  2.5992e-01,\n",
      "         1.9195e-01,  2.0863e-01,  1.7399e-01,  2.2919e-01,  2.9537e-01,\n",
      "         2.7318e-01,  3.4565e-01,  3.7692e-01,  3.5682e-01, -3.0766e-03,\n",
      "         9.6306e-02,  2.4732e-01,  3.7746e-01,  5.1970e-01,  7.0260e-01,\n",
      "         7.9302e-01, -9.7852e-03, -1.0339e-02,  8.3431e-03,  6.7564e-03,\n",
      "         8.0730e-03,  1.2061e-03,  2.6650e-03,  4.6580e-03, -8.6009e-04,\n",
      "         4.0914e-04, -1.6096e-03,  3.7709e-03,  2.7819e-03,  4.1787e-03,\n",
      "         2.1994e-03, -1.0921e-03,  3.4963e-03, -1.5955e-03, -1.0788e-04,\n",
      "         1.2627e-03,  7.4599e-03,  3.5914e-03,  1.4432e-03,  7.5044e-03,\n",
      "         2.1654e-03,  2.5071e-03,  4.6810e-03, -2.7737e-03,  4.6757e-03,\n",
      "        -2.3343e-04,  4.9937e-03, -3.4699e-03,  1.9630e-03,  4.6188e-03,\n",
      "         4.0082e-03,  4.5224e-03,  1.0501e-02,  6.4757e-04,  1.3987e-04,\n",
      "         2.3452e-03,  1.4367e-03,  2.1302e-03, -1.5355e-03, -2.9677e-03,\n",
      "         1.2404e-03,  2.6552e-03,  6.3838e-03,  6.8199e-03, -9.1264e-04,\n",
      "        -1.7859e-03,  2.3270e-03, -1.0621e-03,  1.1232e-03, -2.6768e-03,\n",
      "        -3.7390e-03, -2.2707e-04, -1.2867e-03,  4.8606e-05, -5.8433e-03,\n",
      "        -1.4193e-04,  3.2119e-04,  4.3930e-05, -5.5088e-05, -4.6172e-03,\n",
      "        -2.1870e-03, -1.9739e-03,  3.9957e-03,  1.6634e-03, -7.5415e-04,\n",
      "         4.1042e-03,  4.7839e-03,  1.0949e-02,  2.3897e-03,  4.4635e-03,\n",
      "         5.5652e-04, -1.7638e-03,  5.6300e-03,  3.0859e-04,  5.2595e-03,\n",
      "         1.7911e-03,  1.2550e-03, -3.6730e-03,  2.2197e-03,  2.0883e-03,\n",
      "         1.9523e-03,  2.7595e-03,  3.3626e-04,  6.5137e-03, -7.8670e-04,\n",
      "         1.0958e-03,  3.1785e-03,  2.7697e-03,  4.3830e-03,  3.3718e-03,\n",
      "         2.6967e-03, -2.4190e-03,  6.2477e-03,  5.1750e-03,  3.7184e-03,\n",
      "         2.7762e-04, -9.5836e-04,  2.1731e-03,  2.0456e-03, -3.1981e-03,\n",
      "         8.5467e-04,  1.5496e-03,  2.9872e-04, -2.1146e-03,  1.0896e-03,\n",
      "         9.7958e-04,  4.0454e-03,  2.1075e-03,  4.5323e-03, -1.5058e-03,\n",
      "        -4.6553e-03, -5.1771e-04, -2.2095e-03, -6.2427e-04,  3.1562e-03,\n",
      "         2.5235e-03,  5.1512e-04, -2.1327e-04, -1.1039e-03, -3.0877e-04,\n",
      "        -4.3270e-04,  6.6587e-03, -1.7417e-04, -3.1316e-04,  5.4838e-03,\n",
      "        -4.4790e-03,  2.9065e-04, -2.9212e-04, -1.3295e-03, -3.6373e-03,\n",
      "         3.5278e-03,  3.6635e-04,  7.0085e-03,  2.1587e-03, -2.8734e-05,\n",
      "         2.2305e-05,  3.3242e-04,  1.6758e-03,  1.1664e-04,  5.5366e-03,\n",
      "        -1.3603e-03,  3.0817e-03,  8.4716e-04,  1.5347e-03,  6.5479e-03,\n",
      "        -2.0083e-03,  2.1383e-03,  6.2024e-03,  8.8852e-03,  3.2306e-03,\n",
      "         2.6742e-03,  5.6004e-03,  7.5725e-03,  7.3410e-04,  3.4167e-03,\n",
      "         4.3220e-03,  6.6257e-03,  3.3908e-03,  3.1341e-03,  8.1889e-03,\n",
      "         3.8717e-03,  2.0685e-03,  8.1052e-03,  5.5696e-03,  8.6358e-03,\n",
      "         1.9152e-03,  1.6490e-03, -3.3567e-04,  4.9609e-03,  9.7336e-03,\n",
      "         9.8326e-04,  1.9176e-04,  3.3501e-03,  6.4418e-03,  1.0355e-03,\n",
      "         3.6076e-03, -1.5266e-04, -7.1956e-04,  3.6001e-03,  3.9585e-03,\n",
      "         6.5633e-03,  3.9179e-03,  6.8011e-03, -1.9977e-04,  7.6487e-03,\n",
      "        -1.8874e-03,  4.8064e-03,  6.0876e-04,  1.9062e-04,  5.0038e-03,\n",
      "        -2.2025e-03,  3.1440e-03, -6.2591e-03,  2.4834e-03, -4.6321e-03,\n",
      "         6.0816e-03,  1.9520e-03, -3.5148e-04,  5.3137e-03,  6.6220e-03,\n",
      "        -2.6785e-03,  5.4353e-03, -1.3221e-03, -4.4390e-04,  4.9874e-03,\n",
      "        -5.5611e-03, -1.2969e-03, -6.5053e-04,  2.0927e-03, -1.7771e-04,\n",
      "         9.6944e-04,  1.0761e-03,  4.0177e-03,  3.0313e-03,  2.5204e-03,\n",
      "         3.5603e-03,  1.6259e-03, -1.2519e-03, -4.6197e-03,  2.0902e-03,\n",
      "        -4.7082e-05,  1.9406e-04,  4.2886e-04,  2.4546e-04,  5.0877e-03,\n",
      "         3.3548e-03, -1.3675e-03,  1.6156e-03, -8.7836e-04, -5.4984e-04,\n",
      "        -1.0353e-03,  5.5835e-04,  2.7975e-03,  7.2677e-04,  1.4295e-03,\n",
      "         3.3891e-03, -3.1886e-03,  1.1118e-02,  3.2851e-03,  3.8281e-05,\n",
      "        -1.7405e-03,  5.9590e-04, -2.3101e-03,  1.0242e-03, -2.8894e-03,\n",
      "         1.5416e-03, -4.5929e-04,  1.4537e-03,  3.6180e-03,  4.4840e-03,\n",
      "         3.0979e-03,  3.7872e-03, -5.5659e-04,  1.8253e-03, -4.4238e-04,\n",
      "        -1.9820e-03, -2.0987e-03, -5.9839e-03,  4.3772e-03,  5.0700e-03,\n",
      "         8.5975e-03,  2.8935e-03,  9.4435e-03, -4.8245e-04,  1.0679e-03,\n",
      "         2.3625e-04,  4.8403e-03,  8.5038e-03,  7.8628e-03, -1.7877e-03,\n",
      "         2.4303e-03,  3.0124e-03,  8.9570e-03,  7.5625e-03,  7.0656e-03,\n",
      "         2.4335e-03,  7.7144e-03,  9.6926e-04, -6.1292e-03, -3.2174e-03,\n",
      "         8.1343e-03,  1.9787e-03,  5.9923e-03,  1.5360e-03, -1.3814e-03,\n",
      "         5.3632e-03, -7.4945e-04,  9.4613e-03, -3.7934e-03,  3.0175e-03,\n",
      "         6.9596e-03, -4.2226e-03, -3.5081e-03, -3.7432e-03, -2.9879e-03,\n",
      "         6.0123e-03,  1.5289e-03, -2.4411e-03,  1.8933e-03,  1.0198e-03,\n",
      "        -1.6521e-03, -1.3362e-03, -4.3676e-03, -1.6090e-05,  2.6780e-04,\n",
      "         5.1608e-03,  2.8469e-03, -3.6980e-03,  2.2275e-03,  3.0348e-03,\n",
      "         2.8763e-04,  5.0013e-03, -4.7764e-04, -4.5981e-04, -4.5326e-04,\n",
      "        -2.3185e-03, -9.4154e-04, -2.2675e-03,  3.0827e-03,  2.5505e-03,\n",
      "         2.0960e-03, -1.4426e-03,  1.3269e-03,  1.7574e-03,  3.0761e-03,\n",
      "         3.8872e-03,  2.8945e-03,  4.4172e-03, -8.1464e-04, -2.0634e-03,\n",
      "        -3.5197e-03, -1.8701e-03, -1.0192e-03, -6.8099e-03, -3.6306e-03,\n",
      "         5.3792e-04, -3.6896e-04,  3.4651e-03, -2.6754e-03,  6.8139e-03,\n",
      "         4.2883e-03,  8.8008e-03,  3.2472e-03, -2.5176e-03, -5.7300e-04,\n",
      "         1.6367e-03, -2.2212e-03,  1.5062e-03, -4.0649e-03,  3.8993e-03,\n",
      "         1.6809e-02,  1.8213e-02, -1.7275e-02, -1.2318e-02, -1.5369e-02,\n",
      "        -1.0894e-02, -2.9057e-02,  6.8491e-03, -1.0491e-02,  5.3160e-03,\n",
      "        -9.3319e-03,  3.3052e-03, -5.6681e-03, -1.7836e-02,  3.9090e-03,\n",
      "        -1.5444e-02, -1.1500e-02], device='cuda:1', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_dataloader, 0):\n",
    "    x_p,x_c,y,hid,x_p_len=map(lambda x: x.to('cuda:1'), batch)\n",
    "    speech_batch = []\n",
    "    for i, sent in enumerate(x_p):\n",
    "        for j, word in enumerate(sent):\n",
    "            if j < x_p_len[i]:\n",
    "                speech_batch.append(word[torch.arange(752)].detach().tolist())\n",
    "    speech_batch = torch.tensor(speech_batch, dtype=torch.float32, device='cuda:1')\n",
    "    print(speech_batch[0])\n",
    "    print(ae(speech_batch[0])[1])\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
